{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitkagglenewpipenv345dfe1713484c20b6863d1100cfe670",
   "display_name": "Python 3.6.9 64-bit ('kaggle_new': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model using all possible features\n",
    "# questions arises if we should encode before or after splitting the data\n",
    "# after some research on the internet, most recommend doing the encoding on the training data, and \n",
    "# repeating the steps on for the test data\n",
    "# most exercises on kaggle did the encoding on the whole dataset, then split it\n",
    "# I think there is a possibility of data leackage when encoding on the whole data set!\n",
    "# BUT I get an error fitting the encoding on the training set, then  transforming the test set, since test set my include features not in train set! So I fitted and transformed the encoding on the whole data set\n",
    "# usind data preperation as in the pytorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  loading data\n",
    "csv_ratings='ml-latest-small/ratings.csv'\n",
    "csv_movies='ml-latest-small/movies.csv'\n",
    "def get_data_ratings(csv_ratings,csv_movies):\n",
    "    zf = zipfile.ZipFile('/home/elena/Downloads/ml-latest-small.zip')\n",
    "    # reading ratings file:\n",
    "    r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "    ratings = pd.read_csv(zf.open(csv_ratings), names=r_cols)\n",
    "    m_cols=['movie_id', 'title', 'genre']\n",
    "    movies = pd.read_csv(zf.open(csv_movies), names=m_cols)\n",
    "    # merging ratings and movies\n",
    "    data=pd.merge(ratings,movies,on='movie_id')\n",
    "    zz = zipfile.ZipFile('/home/elena/Downloads/ml-100k.zip')\n",
    "    # reading users file:\n",
    "    u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "    users = pd.read_csv(zz.open('ml-100k/u.user'), sep='|', names=u_cols,encoding='latin-1')\n",
    "    return pd.merge(users,data, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_data_ratings(csv_ratings,csv_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 193609, 9724)"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "unique_movies=data.movie_id.unique()\n",
    "unique_movies.min(),unique_movies.max(), len(unique_movies)\n",
    "# movie_id's are numbers, but they have a bigger range then the actual amount of unique movies!\n",
    "# lets start with indexing the movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train / test data\n",
    "def train_test_data(ratings):\n",
    "    unique_movies = ratings.movie_id.unique() # returns a np array\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)} # indexing movie_id, tart at 0\n",
    "    index_to_movie = {idx: movie for movie, idx in movie_to_index.items()}\n",
    "    new_movies = ratings.movie_id.map(movie_to_index) # replaces movie_id with coresp. index\n",
    "    ratings['movie_index']=new_movies\n",
    "\n",
    "    train=pd.read_pickle('/home/elena/Downloads/traindata.pkl')\n",
    "    test=pd.read_pickle('/home/elena/Downloads/testdata.pkl')\n",
    "    train['movie_index']=train.movie_id.map(movie_to_index)\n",
    "    test['movie_index']=test.movie_id.map(movie_to_index)\n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(610, 1, 610)"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "data.user_id.unique().shape[0], data.user_id.min(), data.user_id.max(), \n",
    "# we have as many users as we have id's for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21, 541, 951)"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "data.occupation.unique().shape[0], data.zip_code.unique().shape[0], data.genre.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have several categorical features; 'sex','occupation','zip_code','genre'\n",
    "# age is the only numerical feature where the number has a 'meaning'\n",
    "# there a 2 ways how to handle cat. features: encoding (different ways to do so) and embeddings\n",
    "#  first approach will be to use embeddings\n",
    "# note: 1 embedding layer is required for each categorical feature, and the embedding expects the \n",
    "# categories to be ordinal encoded, although no relationship between the categories is assumed \n",
    "# (statement from kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to better compare the above models with the ones where only user and movies are used as inputs, we \n",
    "# should fit the encoding to the whole data and then transform it on train and test, since we did that # with the indexing of the movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.rating\n",
    "y_test=test.rating\n",
    "X_train=train.drop('rating',axis=1)\n",
    "X_test=test.drop('rating', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and getting data ready\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 1. approach: we fit on the training data, and transform the encoding to train and test datat \n",
    "# we got an error, because e.g. we have genre which are in test data but not in train!\n",
    "# thus, we use encoding seperately\n",
    "# Make copy to avoid changing original data \n",
    "X_train_label = X_train.copy()\n",
    "X_test_label = X_test.copy()\n",
    "\n",
    "cat_features=['unix_timestamp','sex','occupation','zip_code','genre']\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in cat_features:\n",
    "    label_encoder.fit(data[col])\n",
    "    X_train_label[col+'_label'] = label_encoder.transform(X_train[col])\n",
    "    X_test_label[col+'_label'] = label_encoder.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation \n",
    "from keras.layers import Input, Embedding, Concatenate, Flatten, Dense, Dot, Add, Multiply, Subtract, Average, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features as embedding inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(85041, 85042)"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "X_train_label.unix_timestamp_label.max(),X_test_label.unix_timestamp_label.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(hidden_units, embedding_dim, merging_method):\n",
    "    # Each instance will consist of 7 inputs:\n",
    "    # col_embedding=['user_id', 'movie_index', 'unix_timestamp', 'sex_label', 'occupation_label', 'zip_code_label', 'genre_label']\n",
    "\n",
    "    user_id_input = Input(shape=(1,), name='user_id')\n",
    "    movie_index_input = Input(shape=(1,), name='movie_index')\n",
    "    unix_timestamp_input = Input(shape=(1,), name='unix_timestamp')\n",
    "    sex_input = Input(shape=(1,), name='sex')\n",
    "    occupation_input=Input(shape=(1,), name='occupation')\n",
    "    zip_code_input=Input(shape=(1,), name='zip_code')\n",
    "    genre_input=Input(shape=(1,), name='genre')\n",
    "\n",
    "    # Embeddings\n",
    "    user_embedded = Embedding(data.movie_id.max()+1, embedding_dim, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "    movie_embedded = Embedding(data.movie_index.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='movie_embedding')(movie_index_input)\n",
    "    unix_timestamp_embedded = Embedding(X_test_label.unix_timestamp_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='unix_timestamp_embedding')(unix_timestamp_input)\n",
    "    sex_embedded = Embedding(X_train_label.sex_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='sex_embedding')(sex_input)\n",
    "    occupation_embedded= Embedding(X_train_label.occupation_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='occupation_embedding')(occupation_input)\n",
    "    zipe_code_embedded = Embedding(X_train_label.zip_code_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='zip_code_embedding')(zip_code_input)\n",
    "    genre_embedded = Embedding(X_train_label.genre_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='genre_embedding')(genre_input)\n",
    "    # merging the embeddings\n",
    "    embeddings_cols=[user_embedded, movie_embedded,  unix_timestamp_embedded, zipe_code_embedded, occupation_embedded, genre_embedded]\n",
    "\n",
    "    if merging_method=='concatenate':\n",
    "        merged = Concatenate()(embeddings_cols)\n",
    "    if merging_method=='dot_product':\n",
    "        merged =Dot(name = 'dot_product', normalize = True, axes = 2)(embeddings_cols)\n",
    "    if merging_method=='add':\n",
    "        merged =Add()(embeddings_cols)\n",
    "    if merging_method=='substract':\n",
    "        merged=Subtract()(embeddings_cols)\n",
    "    if merging_method=='multiply':\n",
    "        merged=Multiply()(embeddings_cols)\n",
    "    if merging_method=='average':\n",
    "        merged=Average()(embeddings_cols)\n",
    "    out = Flatten()(merged)\n",
    "\n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        out = Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = Dense(1, activation='linear', name='prediction')(out)\n",
    "    model=Model(inputs = [user_id_input, movie_index_input, unix_timestamp_input, sex_input, occupation_input, zip_code_input, genre_input],outputs = out)\n",
    "    model.compile(optimizer = 'Adam',loss='MSE',metrics=['MAE'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = (100,50) #same as in pytorch model\n",
    "embedding_dim = 50 #same as in pytorch model\n",
    "es=EarlyStopping(monitor='val_MAE', min_delta=0, patience=0, verbose=0, mode='min', baseline=None, restore_best_weights=False)\n",
    "# col_embedding=['user_id', 'movie_index', 'unix_timestamp', 'sex_label', 'occupation_label', #'zip_code_label', 'genre_label']\n",
    "\n",
    "inputs=[X_train_label.user_id,X_train_label.movie_index,X_train_label.unix_timestamp_label,X_train_label.sex_label, X_train_label.occupation_label, X_train_label.zip_code_label, X_train_label.genre_label]\n",
    "inputs_test=[X_test_label.user_id,X_test_label.movie_index,X_test_label.unix_timestamp_label,X_test_label.sex_label, X_test_label.occupation_label, X_test_label.zip_code_label, X_test_label.genre_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergemethod=['concatenate', 'add', 'multiply', 'average']\n",
    "# merging method dot product and substract are only possible for 2 inputs / embedding\n",
    "summary=pd.DataFrame(columns=['merge','val_MAE', 'MAE','epoch','val_loss', 'loss'])\n",
    "merge,epoch,val_MAE,MAE,loss,val_loss=[],[],[],[],[],[]\n",
    "# looping through the merging methods\n",
    "for m in mergemethod:\n",
    "    model=embedding_model(hidden_units, embedding_dim, merging_method=m)\n",
    "    history=model.fit(x=inputs, y=y_train, batch_size=500,epochs=10, verbose=0, validation_data=[inputs_test,y_test], callbacks=[es])\n",
    "    # collecting MAE's and loss\n",
    "    merge.append(m)\n",
    "    n=len(history.epoch)\n",
    "    epoch.append(n)\n",
    "    val_MAE.append(history.history['val_MAE'][n-1])\n",
    "    MAE.append(history.history['MAE'][n-1])\n",
    "    loss.append(history.history['loss'][n-1])\n",
    "    val_loss.append(history.history['val_loss'][n-1])\n",
    "summary['merge']=merge \n",
    "summary['val_MAE']=val_MAE \n",
    "summary['epoch']=epoch\n",
    "summary['MAE']=MAE \n",
    "summary['loss']=loss \n",
    "summary['val_loss']=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         merge   val_MAE       MAE  epoch  val_loss      loss\n0  concatenate  0.697425  0.568162      2  0.818559  0.578214\n1          add  0.687128  0.557094      2  0.795325  0.561479\n2     multiply  0.824976  0.830012      3  1.082283  1.088740\n3      average  0.687514  0.474438      3  0.796303  0.441894",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>merge</th>\n      <th>val_MAE</th>\n      <th>MAE</th>\n      <th>epoch</th>\n      <th>val_loss</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>concatenate</td>\n      <td>0.697425</td>\n      <td>0.568162</td>\n      <td>2</td>\n      <td>0.818559</td>\n      <td>0.578214</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>add</td>\n      <td>0.687128</td>\n      <td>0.557094</td>\n      <td>2</td>\n      <td>0.795325</td>\n      <td>0.561479</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>multiply</td>\n      <td>0.824976</td>\n      <td>0.830012</td>\n      <td>3</td>\n      <td>1.082283</td>\n      <td>1.088740</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average</td>\n      <td>0.687514</td>\n      <td>0.474438</td>\n      <td>3</td>\n      <td>0.796303</td>\n      <td>0.441894</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['user_id', 'age', 'sex', 'occupation', 'zip_code', 'movie_id',\n       'unix_timestamp', 'title', 'genre', 'movie_index',\n       'unix_timestamp_label', 'sex_label', 'occupation_label',\n       'zip_code_label', 'genre_label'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "X_train_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1185, 31422,    14],\n       [ 2266, 72974,    18],\n       [   68, 46607,     0],\n       ...,\n       [ 2290, 33548,    15],\n       [  708,  2135,     6],\n       [ 1312, 59070,    18]])"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = ['user_id', 'movie_index', 'unix_timestamp_label', 'sex_label', 'occupation_label','zip_code_label', 'genre_label']\n",
    "selector = SelectKBest(f_classif, k=3)\n",
    "X_new = selector.fit_transform(X_train_label[feature_cols], y_train)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        user_id  movie_index  unix_timestamp_label  sex_label  \\\n97717         0         1185                 31422          0   \n100124        0         2266                 72974          0   \n25952         0           68                 46607          0   \n25871         0         2183                 29553          0   \n97255         0         1495                 47137          0   \n\n        occupation_label  zip_code_label  genre_label  \n97717                 14               0            0  \n100124                18               0            0  \n25952                  0               0            0  \n25871                 13               0            0  \n97255                  4               0            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_index</th>\n      <th>unix_timestamp_label</th>\n      <th>sex_label</th>\n      <th>occupation_label</th>\n      <th>zip_code_label</th>\n      <th>genre_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97717</th>\n      <td>0</td>\n      <td>1185</td>\n      <td>31422</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>100124</th>\n      <td>0</td>\n      <td>2266</td>\n      <td>72974</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25952</th>\n      <td>0</td>\n      <td>68</td>\n      <td>46607</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25871</th>\n      <td>0</td>\n      <td>2183</td>\n      <td>29553</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97255</th>\n      <td>0</td>\n      <td>1495</td>\n      <td>47137</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        user_id  movie_index  unix_timestamp_label  sex_label  \\\n97717       606         1185                 31422          0   \n100124      610         2266                 72974          0   \n25952       180           68                 46607          0   \n25871       178         2183                 29553          0   \n97255       605         1495                 47137          0   \n\n        occupation_label  zip_code_label  genre_label  \n97717                 14               0            0  \n100124                18               0            0  \n25952                  0               0            0  \n25871                 13               0            0  \n97255                  4               0            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_index</th>\n      <th>unix_timestamp_label</th>\n      <th>sex_label</th>\n      <th>occupation_label</th>\n      <th>zip_code_label</th>\n      <th>genre_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97717</th>\n      <td>606</td>\n      <td>1185</td>\n      <td>31422</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>100124</th>\n      <td>610</td>\n      <td>2266</td>\n      <td>72974</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25952</th>\n      <td>180</td>\n      <td>68</td>\n      <td>46607</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25871</th>\n      <td>178</td>\n      <td>2183</td>\n      <td>29553</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97255</th>\n      <td>605</td>\n      <td>1495</td>\n      <td>47137</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "selector = SelectKBest(f_classif, k=4)\n",
    "X_new = selector.fit_transform(X_train_label[feature_cols], y_train)\n",
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        user_id  movie_index  unix_timestamp_label  sex_label  \\\n97717         0         1185                 31422          0   \n100124        0         2266                 72974          0   \n25952         0           68                 46607          0   \n25871         0         2183                 29553          0   \n97255         0         1495                 47137          0   \n\n        occupation_label  zip_code_label  genre_label  \n97717                  0               0            0  \n100124                 0               0            0  \n25952                  0               0            0  \n25871                  0               0            0  \n97255                  0               0            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_index</th>\n      <th>unix_timestamp_label</th>\n      <th>sex_label</th>\n      <th>occupation_label</th>\n      <th>zip_code_label</th>\n      <th>genre_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97717</th>\n      <td>0</td>\n      <td>1185</td>\n      <td>31422</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>100124</th>\n      <td>0</td>\n      <td>2266</td>\n      <td>72974</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25952</th>\n      <td>0</td>\n      <td>68</td>\n      <td>46607</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25871</th>\n      <td>0</td>\n      <td>2183</td>\n      <td>29553</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97255</th>\n      <td>0</td>\n      <td>1495</td>\n      <td>47137</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "selector = SelectKBest(f_classif, k=2)\n",
    "X_new = selector.fit_transform(X_train_label[feature_cols], y_train)\n",
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important: movie_index, unix_timestamp_label, occupation_label, user_id\n",
    "# let's create an embedding model with movie_index, unix_timestamp_label, occupation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(hidden_units, embedding_dim, merging_method):\n",
    "    # Each instance will consist of 7 inputs:\n",
    "    # col_embedding=['user_id', 'movie_index', 'unix_timestamp', 'sex_label', 'occupation_label', 'zip_code_label', 'genre_label']\n",
    "\n",
    "    movie_index_input = Input(shape=(1,), name='movie_index')\n",
    "    unix_timestamp_input = Input(shape=(1,), name='unix_timestamp')\n",
    "    occupation_input=Input(shape=(1,), name='occupation')\n",
    "\n",
    "    # Embeddings\n",
    "    movie_embedded = Embedding(data.movie_index.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='movie_embedding')(movie_index_input)\n",
    "    unix_timestamp_embedded = Embedding(X_test_label.unix_timestamp_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='unix_timestamp_embedding')(unix_timestamp_input)\n",
    "    occupation_embedded= Embedding(X_train_label.occupation_label.max()+1, embedding_dim, \n",
    "                                        input_length=1, name='occupation_embedding')(occupation_input)\n",
    "    # merging the embeddings\n",
    "    embeddings_cols=[movie_embedded,  unix_timestamp_embedded, occupation_embedded]\n",
    "\n",
    "    if merging_method=='concatenate':\n",
    "        merged = Concatenate()(embeddings_cols)\n",
    "    if merging_method=='dot_product':\n",
    "        merged =Dot(name = 'dot_product', normalize = True, axes = 2)(embeddings_cols)\n",
    "    if merging_method=='add':\n",
    "        merged =Add()(embeddings_cols)\n",
    "    if merging_method=='substract':\n",
    "        merged=Subtract()(embeddings_cols)\n",
    "    if merging_method=='multiply':\n",
    "        merged=Multiply()(embeddings_cols)\n",
    "    if merging_method=='average':\n",
    "        merged=Average()(embeddings_cols)\n",
    "    out = Flatten()(merged)\n",
    "\n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        out = Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = Dense(1, activation='linear', name='prediction')(out)\n",
    "    model=Model(inputs = [movie_index_input, unix_timestamp_input, occupation_input],outputs = out)\n",
    "    model.compile(optimizer = 'Adam',loss='MSE',metrics=['MAE'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[X_train_label.movie_index,X_train_label.unix_timestamp_label, X_train_label.occupation_label]\n",
    "inputs_test=[X_test_label.movie_index,X_test_label.unix_timestamp_label, X_test_label.occupation_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 2 arrays: [array([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       ...",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-8096e764ee67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmovies_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'merge'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_MAE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/kaggle_new-dNsOYfOQ/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/kaggle_new-dNsOYfOQ/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/kaggle_new-dNsOYfOQ/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 2 arrays: [array([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       ..."
     ]
    }
   ],
   "source": [
    "mergemethod=['concatenate', 'add', 'multiply', 'average']\n",
    "# merging method dot product and substract are only possible for 2 inputs / embedding\n",
    "summary=pd.DataFrame(columns=['merge','val_MAE', 'MAE','epoch','val_loss', 'loss'])\n",
    "merge,epoch,val_MAE,MAE,loss,val_loss=[],[],[],[],[],[]\n",
    "# for prediction\n",
    "# looping through the merging methods\n",
    "for m in mergemethod:\n",
    "    model=embedding_model(hidden_units, embedding_dim, merging_method=m)\n",
    "    history=model.fit(x=inputs, y=y_train, batch_size=500,epochs=10, verbose=0, validation_data=[inputs_test,y_test], callbacks=[es])\n",
    "    # collecting MAE's and loss\n",
    "    merge.append(m)\n",
    "    n=len(history.epoch)\n",
    "    epoch.append(n)\n",
    "    val_MAE.append(history.history['val_MAE'][n-1])\n",
    "    MAE.append(history.history['MAE'][n-1])\n",
    "    loss.append(history.history['loss'][n-1])\n",
    "    val_loss.append(history.history['val_loss'][n-1])\n",
    "summary['merge']=merge \n",
    "summary['val_MAE']=val_MAE \n",
    "summary['epoch']=epoch\n",
    "summary['MAE']=MAE \n",
    "summary['loss']=loss \n",
    "summary['val_loss']=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         merge   val_MAE       MAE  epoch  val_loss      loss\n0  concatenate  0.764004  0.605479      2  0.949332  0.641580\n1          add  0.762126  0.588238      2  0.951217  0.612642\n2     multiply  0.855141  0.642427      3  1.137048  0.748072\n3      average  0.748042  0.610416      2  0.913490  0.660176",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>merge</th>\n      <th>val_MAE</th>\n      <th>MAE</th>\n      <th>epoch</th>\n      <th>val_loss</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>concatenate</td>\n      <td>0.764004</td>\n      <td>0.605479</td>\n      <td>2</td>\n      <td>0.949332</td>\n      <td>0.641580</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>add</td>\n      <td>0.762126</td>\n      <td>0.588238</td>\n      <td>2</td>\n      <td>0.951217</td>\n      <td>0.612642</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>multiply</td>\n      <td>0.855141</td>\n      <td>0.642427</td>\n      <td>3</td>\n      <td>1.137048</td>\n      <td>0.748072</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average</td>\n      <td>0.748042</td>\n      <td>0.610416</td>\n      <td>2</td>\n      <td>0.913490</td>\n      <td>0.660176</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "summary # worse then using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}